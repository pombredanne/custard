<nav class="well">
  <ul class="nav nav-list">
    <li class="nav-header">Overview</li>
    <li><a data-nonpushstate href="#intro">Introduction</a></li>
    <li><a data-nonpushstate href="#type">Type of cloud service</a></li>
    <li><a data-nonpushstate href="#what">Languages / protocols</a></li>
    <li class="nav-header">Continuity of service</li>
    <li><a data-nonpushstate href="#backups">Backups / disaster recovery</a></li>
    <li><a data-nonpushstate href="#leaving">Should you wish to leave</a></li>
  </ul>
</nav>

<div class="wrapper">

<h2 id="intro">ScraperWiki for Corporates</h2>

<p>ScraperWiki helps people manage datasets, and the tools needed to get, clean
and analyse data.</p>

<p>This page describes the <b>corporate</b> version of ScraperWiki, covering
matters relevant to legal, IT and financial staff. Levels of service for
non-corporate level customers are lower, and are described in separate terms
and conditions. End user documentation is also separate.</p>

<p>We undertake to describe all aspects of our service clearly and
transparently. If you have any questions, please get in touch and help us 
improve this living document.</p>

<hr />

<h2 id="type">Type of cloud service</h2>

<p>ScraperWiki is a hybrid of <abbr title="Software as a Service">SaaS</abbr> and <abbr title="Platform as a Service">PaaS</abbr>. It provides end user tools like a
SaaS, yet allows visibility and customisation of all source code like a PaaS.
This is because data tools need to be <b>easy</b> for anyone to use in most cases, yet
<b>powerful</b> enough for those who can program to handle edge cases when needed.
</p>

<p>ScraperWiki is deployed in a Public Cloud model. It is hosted on <abbr title="Infrastructure as a Service">IaaS</abbr>
servers at <a href="https://aws.amazon.com/">Amazon Web Services</a> in the US.
We meet Amazon's tiering standards.
</p>

<p>We currently scale by provisioning as required, as we are a platform for
collaboration on (relatively) small amounts of data. We outsource any very
large data volumes to other providers. By default we offer non-guaranteed
performance, we can offer guaranteed for a larger cost.  Utilisation reporting,
where there are restrictions, is built into the platform and its API.
</p>

<p>Our platform can be freely used by our customers in any way they like. In
particular, customers can contract third parties to adapt, extend or write new
data tools and applications which run on the platform, as they want. Even
the code of the platform itself is open source.
</p>

<hr />

<h2 id="what">Languages / protocols</h2>

<p>ScraperWiki is a web platform for data science. Subscribers to ScraperWiki can
run scripts which get, clean and analyse data &ndash; such as scrapers and
visualisations. It can be used to gather information from disparate sources,
including websites, FTP servers, legacy database systems, PDFs, and then export
that in a common format (Excel, JSON, CSV), or as interactive web
visualisations.
</p>

<p>ScraperWiki supports:</p>
<ul>
  <li>Any open source programming language &ndash; Python, R, Perl, Java and so on.</li>
  <li>Schema-light SQL datasets (based on SQLite or PostgreSQL).</li>
  <li>Integration between datasets and views using web-standards based SQL API, which returns JSON over a REST interface.</li>
  <li>Full POSIX filesystem.</li>
  <li>External integration via SSH, SFTP, SCP, Git, Subversion and other standard Unix tools.</li>
  <li>Data export over HTTP, data visualisation using HTML/Javascript working with any of the backend languages.</li>
</ul>

<p class="well well-small"><span class="label label-info">Good to know!</span> Each dataset is a full virtual Linux shell account, meaning you can run any other Unix software you like, including binary-only software (e.g. Abbyy's PDF reader).</p>

<hr />

<h2 id="backups">Backups / disaster recovery</h2>

<p>All dataset files and data are stored on Amazon Elastic Block Store (EBS)
volumes. Metadata is stored in a MongoDB cluster on MongoHQâ€™s cloud service.
</p>

<p>These are five sequentially worse disaster situations, with recovery
times indicated.</p>

<style type="text/css">
.disasters td {
  vertical-align: middle;
}
td.disaster-summary,
td.disaster-estimate {
  font-weight: bold;
  text-align: center;
}
td.disaster-description {
  font-size: 14px;
  line-height: 20px;
}
.troublesome td {
  background-color: #FFFAE8;
  border-color: #E9E1C3;
}
.bad td {
  background-color: #FFE5C5;
  border-color: #E9CBA7;
}
.terrible td {
  background-color: #FFC793;
  border-color: #E6A971;
}
</style>

<table class="table table-bordered disasters">
  <tr>
    <td class="disaster-summary">Storage hardware failure</td>
    <td class="disaster-description">
      We store all important data on EBS, which is highly reliable. Volumes are
      automatically replicated so they are not vulnerable to failure of any single
      hardware component.
    </td>
    <td class="disaster-estimate">No time</td>
  </tr>
  <tr class="troublesome">
    <td class="disaster-summary">Single machine failure</td>
    <td class="disaster-description">We can automatically rebuild individual machines in about <b>20 minutes</b>, and the whole cluster in under <b>2 hours</b>.
      We mount the EBS image from the failed machine on the new machine, so no data is lost. 
      MongoDB is backed up to Amazon S3, where we can restore in under <b>1 hour</b>.</td>
    <td class="disaster-estimate">~1&nbsp;hour</td>
  </tr>
  <tr class="bad">
    <td class="disaster-summary">Filesystem or EBS failure</td>
    <td class="disaster-description">
      Should an EBS volume fail or if the files on it accidentally damaged, we
      will recover it from daily, rotating snapshots.  These are kept in two separate
      Amazon regions. We can restore the whole filesystem in under <b>1
      day</b>, and corporate plan customers in under <b>2 hours</b>.</td>
    <td class="disaster-estimate">&lt;2&nbsp;hours</td>
  </tr>
  <tr class="terrible">
    <td class="disaster-summary">Datacentre failure</td>
    <td class="disaster-description">Should the Amazon US data centre we use fail, they have additional data centres which we can restore to in under <b>1 day</b> for all users, <b>under 2 hours</b> for corporate customers.</td>
    <td class="disaster-estimate">&lt;1&nbsp;day</td>
  </tr>
</table>


<hr />

<h2 id="leaving">Should you wish to leave</h2>

<p>We try to keep your custom by offering excellent service, not by locking you
in.</p>

<p>Your data and files are available programmatically at all times, via our
APIs and using the SFTP file transfer protocol. You can use this to make your
own backups, or to migrate data off ScraperWiki. There is no extra charge for this.
Scrapers and views are written using standard open source tools and
protocols, making it relatively easy to migrate individual applications to your
own Linux servers.</p>

<p>In addition, the ScraperWiki platform itself and the core tools are all open
source (see our <a href="http://github.com/scraperwiki">GitHub account</a>).
This gives you the ultimate protection of being able to host it yourself, or 
pay another organisation to host it for you.
</p>

<p class="well well-small"><span class="label label-important">Purge data!</span> 
Should it be required for compliance or other reasons, data can be 
destroyed. This can be done at any time by administrators of the account, or by
asking us to. It will remain in backups until they are rotated out.
</p>

<!-- TODO: -->

<!-- For current status, and uptime history, see our server monitoring at 
<a href="http://status.scraperwiki.com/">status.scraperwiki.com</a>. -->

<!-- Ordering, invoicing and on boarding - how to place order, and set up admin
users and so on -->

<!-- Information assurance - legal requirements -->

<!-- Termination - how to terminate, typically year notice -->

<!-- Pricing, including trial -->

<!-- Service constraints - Maintenance window, customisability, coping with deprecation -->
<!-- Our platform is completely customisable. End users can see the source code
of the data tools, and alter, fork and improve them. You can do what you like.

Old tools are not directly deprecated, they will continue to work as each place
they are used there is a fork of the code. Platform facilities will generally
be altered in such a way as to not alter the function of existing tools. Where
platform features need to be deprecated, end users will be given six months
notice, and help migrating any old tools they have which rely on the changing
features.  -->

<!-- Documentation, tutorials and training 
ScraperWiki is open source, and has a thriving community. This supplements paid
for support. Questions can be asked and answered on our public data hub, on a
public mailing list (Google Groups) and on StackOverflow (programming Q&A
site). Quick and diverse help is immediately available for free from those
routes.

The platform's special features are fully documented. Everything else uses
standard Unix and open source tools, which are all documented by their upstream
projects, which have their own help and support groups.
-->

<!-- Service Levels inc. compensation -->

</div>
